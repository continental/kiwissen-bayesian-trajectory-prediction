'''
Dataloader for NuScenes data

Copyright (c) 2021-2022 Continental AG.

@author: Christian Wirth
'''
from builtins import property
from functools import reduce, partial
import multiprocessing
import os
from pathlib import Path
import pdb
import pickle

import cv2
import gin
from matplotlib import pyplot as plt
from nuscenes import NuScenes
from nuscenes.eval.common.utils import quaternion_yaw
from nuscenes.eval.prediction.splits import get_prediction_challenge_split
from nuscenes.map_expansion.map_api import NuScenesMap
from nuscenes.prediction import PredictHelper
from nuscenes.prediction.helper import convert_local_coords_to_global
from nuscenes.prediction.input_representation.agents import AgentBoxesWithFadedHistory
from nuscenes.prediction.input_representation.combinators import (
    Rasterizer,
    add_foreground_to_image,
)
from nuscenes.prediction.input_representation.interface import InputRepresentation
from nuscenes.prediction.input_representation.static_layers import StaticLayerRasterizer
from nuscenes.prediction.input_representation.utils import (
    convert_to_pixel_coords,
    get_rotation_matrix,
    get_crops,
)
from pyquaternion.quaternion import Quaternion
from shapely.geometry import Polygon, MultiPolygon, LineString, Point, box

import numpy as np
import tensorflow as tf
import pickle


def read_cached_parameterized(filename_fn, shape_fn, dtype=np.float32):
    """
    Generic caching wrapper, using Numpy MEM and pickle files. Caches the output to filename_fn, assuming the name is unique.
    Pickle used for object dtype, mem for everything else.

    :param callable filename_fn: Function returning a unique filename, based on instance_token and sample_token
    :param callable shape_fn: Returns a shape tuple, based on self. May return None for object dtype.
    :param type dtype: dtype of the function result
    :return: The data generated by the wrapped function
    :rtype: np.ndarray or object
    """

    def read_cached(create_fn):
        def inner(self, **kwargs):
            filename = filename_fn(self, **kwargs)
            shape = shape_fn(self)
            file = (
                self.cache_dir / f'{filename}.pkl'
                if dtype == object
                else self.cache_dir / f'{filename}.mem'
            )
            if self.cache_dir is None or not file.exists():
                data = create_fn(self, **kwargs)
                if self.cache_dir is not None:
                    if dtype != object:
                        fmem = np.memmap(
                            file,
                            dtype=dtype,
                            mode='w+',
                            shape=shape,
                        )
                        fmem[:] = data.astype(dtype)[:]
                    else:
                        with open(file, 'wb') as filestream:
                            pickle.dump(data, filestream)
            else:
                if dtype != object:
                    data = (
                        np.memmap(
                            file,
                            dtype=dtype,
                            mode='r',
                            shape=shape,
                        )
                        .copy()
                        .astype(dtype)
                    )
                else:
                    with open(file, 'rb') as filestream:
                        data = pickle.load(filestream)
            return data

        return inner

    return read_cached


@gin.configurable
class NuscenesDataset:
    """
    NuScenes dataset with preprocessing pipeline

    :param str split: train, train_val or val
    :oaram str data_dir: dataset root folder
    :param str,None cache_dir: Optional. Caching directory
    :param str data_version: NuScenes dataset version name
    :param int,str limit: Limit in terms of instances of map name
    :param int eps_set: Covernet epsilon value for trajectory set
    :param bool y_all_valid: If True, y will be all valid trajectories, instead of observed
    :param bool multitask: If True, observed and valid trajectories will be returned
    :paran mp_pool: Optional. Multiprocessing pool for reuse
    """

    def __init__(
        self,
        split,
        data_dir='D:/nuScenes_mini',
        cache_dir='cache',
        data_version='v1.0-mini',
        limit=-1,
        eps_set=4,
        y_all_valid=False,
        multitask=False,
        mp_pool=None,
    ):
        # tf.data.experimental.enable_debug_mode()

        self.data_dir = data_dir
        self.data_version = data_version
        self._helper = None
        self._mtp_input_representation = None
        self._trajectories_ego = None
        self._maps = None

        self.split = split

        self.token_pairs_full = get_prediction_challenge_split(
            split, dataroot=self.data_dir
        )
        self.token_pairs_full = [pair.split("_") for pair in self.token_pairs_full]

        agent_rasterizer = AgentBoxesWithFadedHistory(
            None,
            seconds_of_history=1,
            meters_ahead=39,
            meters_behind=9,
            meters_left=24,
            meters_right=24,
        )
        self.image_size = (
            int(
                (agent_rasterizer.meters_left + agent_rasterizer.meters_right)
                / agent_rasterizer.resolution
            ),
            int(
                (agent_rasterizer.meters_left + agent_rasterizer.meters_right)
                / agent_rasterizer.resolution
            ),
            3,
        )

        if cache_dir is not None:
            cache_dir = Path(cache_dir)
            if not cache_dir.is_absolute():
                root = Path(os.path.dirname(os.path.realpath(__file__))).parent.parent
                cache_dir = root / cache_dir
            self.cache_dir = cache_dir / f'{self.image_size[0]}_{self.image_size[1]}'

            self.cache_dir.mkdir(parents=True, exist_ok=True)
        else:
            self.cache_dir = None

        self.num_future_sec = 6

        self.eps_set = eps_set
        self.y_all_valid = y_all_valid
        self.multitask = multitask

        self.mp_pool = mp_pool
        self.driveable_polygons = {}

        if isinstance(limit, str):
            mapping = self.get_map_mappings()
            self.token_pairs = [
                (instance_token, sample_token)
                for instance_token, sample_token in self.token_pairs_full
                if mapping[sample_token] == limit
            ]
        elif isinstance(limit, float) and limit>0.0 and limit<=1.0: # limit given as percentage
            limit = int(limit*len(self.token_pairs_full))
            self.token_pairs = self.token_pairs_full[:limit]
        else:
            self.token_pairs = self.token_pairs_full[:limit]

    # =========================================================================
    #         instance_token = 'bc38961ca0ac4b14ab90e547ba79fbb6'
    #         sample_token = 'e0845f5322254dafadbbed75aaa07969'
    #
    #         img = self.mtp_input_representation.make_input_representation(instance_token, sample_token)
    #         buffer = max([agent_rasterizer.meters_ahead, agent_rasterizer.meters_behind,
    #                       agent_rasterizer.meters_left, agent_rasterizer.meters_right]) * 2
    #         image_side_length = int(buffer/agent_rasterizer.resolution)
    #         agent_pixels = int(image_side_length / 2), int(image_side_length / 2)
    #         agent_image = np.zeros((image_side_length,image_side_length, 3))
    #         starting_annotation = self.helper.get_sample_annotation(instance_token, sample_token)
    #         center_agent_yaw = quaternion_yaw(Quaternion(starting_annotation['rotation']))
    #         rotation_mat = get_rotation_matrix(agent_image.shape, center_agent_yaw)
    # =========================================================================

    # ======================================================================
    # import cProfile, pstats, io
    # from pstats import SortKey
    # pr = cProfile.Profile()
    # pr.enable()
    # ======================================================================

    # ======================================================================
    # trajectories_taken = self.read_target_trajectories(instance_token, sample_token)
    # trajectories_valid = self.read_possible_trajectories(instance_token, sample_token)
    # ======================================================================

    # ======================================================================
    # pr.disable()
    # s = io.StringIO()
    # sortby = SortKey.CUMULATIVE
    # ps = pstats.Stats(pr, stream=s).sort_stats(sortby)
    # ps.print_stats()
    # print(s.getvalue())
    # ======================================================================

    # =========================================================================
    #         for trajectory in self.trajectories_ego:
    #             trajectory_global = convert_local_coords_to_global(trajectory, starting_annotation['translation'], starting_annotation['rotation'])
    #             for p1, p2 in zip(trajectory_global[:-1],trajectory_global[1:]):
    #                 p1 = convert_to_pixel_coords(p1,starting_annotation['translation'][:2],agent_pixels,agent_rasterizer.resolution)
    #                 p2 = convert_to_pixel_coords(p2,starting_annotation['translation'][:2],agent_pixels,agent_rasterizer.resolution)
    #                 cv2.line(agent_image, (p1[1],p1[0]), (p2[1],p2[0]), color=(255,0,0), thickness=10)
    #
    #         for trajectory in trajectories_valid:
    #             trajectory_global = convert_local_coords_to_global(trajectory, starting_annotation['translation'], starting_annotation['rotation'])
    #             for p1, p2 in zip(trajectory_global[:-1],trajectory_global[1:]):
    #                 p1 = convert_to_pixel_coords(p1,starting_annotation['translation'][:2],agent_pixels,agent_rasterizer.resolution)
    #                 p2 = convert_to_pixel_coords(p2,starting_annotation['translation'][:2],agent_pixels,agent_rasterizer.resolution)
    #                 cv2.line(agent_image, (p1[1],p1[0]), (p2[1],p2[0]), color=(255,255,0), thickness=10)
    #
    #         for trajectory in trajectories_taken:
    #             trajectory_global = convert_local_coords_to_global(trajectory, starting_annotation['translation'], starting_annotation['rotation'])
    #             for p1, p2 in zip(trajectory_global[:-1],trajectory_global[1:]):
    #                 p1 = convert_to_pixel_coords(p1,starting_annotation['translation'][:2],agent_pixels,agent_rasterizer.resolution)
    #                 p2 = convert_to_pixel_coords(p2,starting_annotation['translation'][:2],agent_pixels,agent_rasterizer.resolution)
    #                 cv2.line(agent_image, (p1[1],p1[0]), (p2[1],p2[0]), color=(0,255,0), thickness=10)
    #
    #         rotated_img = cv2.warpAffine(agent_image, rotation_mat, agent_image.shape[:2]).astype('uint8')
    #         row_crop, col_crop = get_crops(agent_rasterizer.meters_ahead, agent_rasterizer.meters_behind,
    #                                    agent_rasterizer.meters_left, agent_rasterizer.meters_right, agent_rasterizer.resolution,
    #                                    image_side_length)
    #         rotated_img = rotated_img[row_crop, col_crop]
    #         img = reduce(add_foreground_to_image, [img,rotated_img])
    #         plt.imshow(img)
    # =========================================================================

    @staticmethod
    def is_possible_trajectory(trajectory, translation, rotation, polygons):
        """
        Determines if all keypoints of the trajectory are within the polygons area.

        :param list trajectory: List of 2D points
        :param translation: Translation matrix local to global
        :param rotation: Rotation matrix local to global
        :param list: List of polygons
        :return: True, all points within area
        :rtype: bool
        """
        trajectory_global = convert_local_coords_to_global(
            trajectory,
            translation,
            rotation,
        )
        for point in trajectory_global:
            if not NuscenesDataset.point_on_polygons(*point, polygons):
                return False
        return True

    @staticmethod
    def point_on_polygons(x: float, y, polygons):
        """
        Determines if a single point is within the polygons area.

        :param float x: X coordinate
        :param float y: Y coordinate
        :param list: List of polygons
        :return: True, all points within area
        :rtype: bool
        """
        point = Point(x, y)

        for polygon in polygons:
            if point.within(polygon):
                return True
            else:
                pass

        # If nothing is found, return an empty string.
        return False

    @read_cached_parameterized(
        lambda self: f'map_mapping_{self.split}',
        shape_fn=lambda self: None,
        dtype=object,
    )
    def get_map_mappings(self):
        """
        Creates a map from sample_token ids to map name

        :return: The mapping
        :rtype: Dict
        """
        sample_tokens = set(
            [sample_token for instance_token, sample_token in self.token_pairs_full]
        )
        map = {
            sample_token: self.helper.get_map_name_from_sample_token(sample_token)
            for sample_token in sample_tokens
        }
        return map

    @property
    def maps(self):
        if not self._maps:
            self._maps = {}
            self._maps['boston-seaport'] = NuScenesMap(
                map_name='boston-seaport', dataroot=self.data_dir
            )
            self._maps['singapore-hollandvillage'] = NuScenesMap(
                map_name='singapore-hollandvillage', dataroot=self.data_dir
            )
            self._maps['singapore-onenorth'] = NuScenesMap(
                map_name='singapore-onenorth', dataroot=self.data_dir
            )
            self._maps['singapore-queenstown'] = NuScenesMap(
                map_name='singapore-queenstown', dataroot=self.data_dir
            )
        return self._maps

    @property
    def trajectories_ego(self):
        if self._trajectories_ego is None:
            root = Path(os.path.dirname(os.path.realpath(__file__))).parent.parent
            with open(root / "data" / f'epsilon_{self.eps_set}.pkl', 'rb') as file:
                trajectories = pickle.load(file)
            self._trajectories_ego = np.asarray(trajectories)
        return self._trajectories_ego

    @property
    def helper(self):
        if not self._helper:
            nuscenes = NuScenes(self.data_version, dataroot=self.data_dir)
            self._helper = PredictHelper(nuscenes)
        return self._helper

    @property
    def mtp_input_representation(self):
        if not self._mtp_input_representation:
            static_layer_rasterizer = StaticLayerRasterizer(
                self.helper,
                meters_ahead=39,
                meters_behind=9,
                meters_left=24,
                meters_right=24,
            )
            agent_rasterizer = AgentBoxesWithFadedHistory(
                self.helper,
                seconds_of_history=1,
                meters_ahead=39,
                meters_behind=9,
                meters_left=24,
                meters_right=24,
            )
            self._mtp_input_representation = InputRepresentation(
                static_layer_rasterizer, agent_rasterizer, Rasterizer()
            )
        return self._mtp_input_representation

    @read_cached_parameterized(
        lambda self, instance_token, sample_token: f'{instance_token}_{sample_token}_gt',
        shape_fn=lambda self: (1, self.num_future_sec * 2, 2),
    )
    def read_target_trajectories(self, instance_token, sample_token):
        """
        Computes the future trajectory for the ego vehicle

        :param str instance_token: Object instance
        :param str sample_token: Scene sample instance
        :return: The future trajectory in ego coordinates
        :rtype: np.ndarray
        """
        future_for_agent = np.expand_dims(
            self.helper.get_future_for_agent(
                instance_token,
                sample_token,
                self.num_future_sec,
                in_agent_frame=True,
                just_xy=True,
            ),
            0,
        )
        return future_for_agent

    @read_cached_parameterized(
        lambda self, instance_token, sample_token: f'{instance_token}_{sample_token}_{self.eps_set}_possible',
        shape_fn=lambda self: self.trajectories_ego.shape,
    )
    def read_possible_trajectories(self, instance_token, sample_token):
        """
        Computes the all possible trajectories for the ego vehicle, based on driveable area.

        :param str instance_token: Object instance
        :param str sample_token: Scene sample instance
        :return: The future trajectories in ego coordinates. Invalid trajectories have 0 values.
        :rtype: np.ndarray
        """
        map_name = self.helper.get_map_name_from_sample_token(sample_token)

        if map_name not in self.driveable_polygons:
            nusc_map = self.maps[map_name]
            records = nusc_map.explorer.map_api.drivable_area
            polygons = [
                nusc_map.explorer.extract_polygon(polygon_token)
                for record in records
                for polygon_token in record['polygon_tokens']
            ]
            self.driveable_polygons[map_name] = polygons
        else:
            polygons = self.driveable_polygons[map_name]

        starting_annotation = self.helper.get_sample_annotation(
            instance_token, sample_token
        )

        trajectory_global_start = convert_local_coords_to_global(
            self.trajectories_ego[0][0:1],
            starting_annotation['translation'],
            starting_annotation['rotation'],
        )

        polygons = sorted(
            polygons,
            key=lambda item: item.distance(
                Point(trajectory_global_start[0, 0], trajectory_global_start[0, 1])
            ),
        )

        if self.mp_pool is None:
            pool = multiprocessing.Pool(processes=multiprocessing.cpu_count())
        else:
            pool = self.mp_pool
        possible = pool.map(
            partial(
                NuscenesDataset.is_possible_trajectory,
                translation=starting_annotation['translation'],
                rotation=starting_annotation['rotation'],
                polygons=polygons,
            ),
            self.trajectories_ego,
        )
        if self.mp_pool is None:
            pool.terminate()
        possible_trajectories = (
            self.trajectories_ego
            * np.asarray(possible, dtype=np.float32)[:, np.newaxis, np.newaxis]
        )
        return possible_trajectories

    @read_cached_parameterized(
        lambda self, instance_token, sample_token: f'{instance_token}_{sample_token}',
        shape_fn=lambda self: self.image_size,
        dtype=np.uint8,
    )
    def read_image(self, instance_token, sample_token):
        """
        Computes the MTP/Covernet image representation of the current scene

        :param str instance_token: Object instance
        :param str sample_token: Scene sample instance
        :return: The RBG image
        :rtype: np.ndarray
        """
        image = self.mtp_input_representation.make_input_representation(
            instance_token, sample_token
        )
        return image

    @read_cached_parameterized(
        lambda self, instance_token, sample_token: f'{instance_token}_{sample_token}_state',
        shape_fn=lambda self: (3),
    )
    def read_agent_state(self, instance_token, sample_token):
        """
        Computes the ego state vector with velocity, acceleation and heading

        :param str instance_token: Object instance
        :param str sample_token: Scene sample instance
        :return: The current stae
        :rtype: np.ndarray
        """
        agent_state_vector = np.asarray(
            [
                self.helper.get_velocity_for_agent(instance_token, sample_token),
                self.helper.get_acceleration_for_agent(instance_token, sample_token),
                self.helper.get_heading_change_rate_for_agent(
                    instance_token, sample_token
                ),
            ]
        )
        return agent_state_vector

    def get_dataset(self, shuffle=False):
        """
        Returns the dataset, as specified by the constructor.

        :param bool shuffle: Dataset is shuffled
        :return: The dataset
        :rtype: tf.data.Dataset
        """
        
        instance_tokens = []
        sample_tokens = []
        for instance_token, sample_token in self.token_pairs:
            instance_tokens.append(instance_token)
            sample_tokens.append(sample_token)

        def read_data(instance_token, sample_token):
            instance_token = instance_token.numpy().decode()
            sample_token = sample_token.numpy().decode()
            # pydevd.settrace(suspend=True)
            image = self.read_image(
                instance_token=instance_token, sample_token=sample_token
            )
            agent_state_vector = self.read_agent_state(
                instance_token=instance_token, sample_token=sample_token
            )
            agent_state_vector[np.isnan(agent_state_vector)] = 0
            future_for_agent_valid = self.read_possible_trajectories(
                instance_token=instance_token, sample_token=sample_token
            )
            
            # if self.multitask and not self.y_all_valid:
            #     future_for_agent_observed = self.read_target_trajectories(
            #         instance_token=instance_token, sample_token=sample_token
            #     )
            #     return image, agent_state_vector, future_for_agent_observed, future_for_agent_valid
            # elif self.multitask and self.y_all_valid:
            #     return image, agent_state_vector, future_for_agent_valid
            # elif not self.multitask and self.y_all_valid:
            #     return image, agent_state_vector, future_for_agent_valid
            # else:
            #     future_for_agent_observed = self.read_target_trajectories(
            #         instance_token=instance_token, sample_token=sample_token
            #     )
            #     return image, agent_state_vector, future_for_agent_observed, future_for_agent_valid
            if self.y_all_valid:
                return image, agent_state_vector, future_for_agent_valid
            else:
                future_for_agent_observed = self.read_target_trajectories(
                    instance_token=instance_token, sample_token=sample_token
                )
                return image, agent_state_vector, future_for_agent_observed, future_for_agent_valid

        instance_dataset = tf.data.Dataset.from_tensor_slices(instance_tokens)
        sample_dataset = tf.data.Dataset.from_tensor_slices(sample_tokens)
        dataset = tf.data.Dataset.zip((instance_dataset, sample_dataset))
        if shuffle:
            dataset = dataset.shuffle(len(self.token_pairs))
        
        if self.multitask and not self.y_all_valid:
            dataset = dataset.map(
                lambda instance_token, sample_token: tf.py_function(
                    read_data,
                    inp=[instance_token, sample_token],
                    Tout=[tf.float32, tf.float32, tf.float32, tf.float32],
                )
            ).map(
                lambda image, agent_state_vector, future_for_agent_observed, future_for_agent_valid: (
                    {'image': image, 'state': agent_state_vector},
                    {'cls': future_for_agent_observed, 'cls_for_label': future_for_agent_valid, 'label': future_for_agent_valid} # have to correspond to heads!?
                )
            )
        elif self.multitask and self.y_all_valid:
            dataset = dataset.map(
                lambda instance_token, sample_token: tf.py_function(
                    read_data,
                    inp=[instance_token, sample_token],
                    Tout=[tf.float32, tf.float32, tf.float32],
                )
            ).map(
                lambda image, agent_state_vector, future_for_agent_valid: (
                    {'image': image, 'state': agent_state_vector},
                    {"cls": future_for_agent_valid, "cls_for_label": future_for_agent_valid, "label": future_for_agent_valid}
                )
            )
        elif not self.multitask and self.y_all_valid:
            dataset = dataset.map(
                lambda instance_token, sample_token: tf.py_function(
                    read_data,
                    inp=[instance_token, sample_token],
                    Tout=[tf.float32, tf.float32, tf.float32],
                )
            ).map(
                lambda image, agent_state_vector, future_for_agent_valid: (
                    {'image': image, 'state': agent_state_vector},
                    future_for_agent_valid,
                )
            )
        else: # case not multitask and not y_all_valid
            dataset = dataset.map(
                lambda instance_token, sample_token: tf.py_function(
                    read_data,
                    inp=[instance_token, sample_token],
                    Tout=[tf.float32, tf.float32, tf.float32, tf.float32],
                )
            ).map(
                lambda image, agent_state_vector, future_for_agent_observed, future_for_agent_valid: (
                    {'image': image, 'state': agent_state_vector},
                    {'cls': future_for_agent_observed, 'cls_for_label': future_for_agent_valid} # have to correspond to heads!?
                )
            )
        return dataset
